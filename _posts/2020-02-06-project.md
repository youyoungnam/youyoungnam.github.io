---
title: 코로나바이러스 자연어처리 프로젝트-1
categories: Project
author_profile: true
---
### 들어가기전에
---
내가 왜 코로나바이러스에 대한 자연어처리 프로젝트를 시작했는지를 말해보려고 한다. 현재 자연어처리 공부한지 얼마 되지않았고, 좀더 나아가고 싶어서 간단한 프로젝트라도 시도 해봤다.

---
### 코로나바이러스 다음 뉴스기사 크롤링(Crawling)
일단, 프로젝트 주제 코로나바이러스에 대한 기사제목 과 언론사의 데이터가 필요했다.

>


**크롤링을 하기위한 라이브러리**
```python
from bs4 import BeautifulSoup
from selenium import webdriver
import urllib
from urllib.request import urlopen
import requests
import time
import pickle
```
한페이지에 10개? 정도의 기사가 있는데 10개의 기사제목과 각 언론사를 가져오고 싶었다.

그래서 총 20 페이지를 크롤링해서 200개 기사제목과 언론사를 크롤링을 했다.

일단, 페이지가 넘어가면서 url이 어떤식으로 변하는지 알아야했다. 그래서 밑에 이미지를보면 
<img src="/assets/images/코로나.png">

URL을 보자
**https://search.daum.net/search?w=news&nil_search=btn&DA=NTB&enc=utf8&cluster=y&cluster_page=1&q=%EC%BD%94%EB%A1%9C%EB%82%98%EB%B0%94%EC%9D%B4%EB%9F%AC%EC%8A%A4**

cluster_page=1& << page=1 라고 써있는부분이 페이지가 넘어갈때마다 변하는걸 알수 있었다.

예를들어, page=1 page=2, page=3 .... 이런식으로 페이지가 넘어 가는걸 알 수 있다.

그다음 어느부분을 크롤링을 해야하는지 봐야한다
<img src= "/assets/images/코로나2.png">
**이렇게 보려면 마우스 우클릭 후 검사를 클릭 하거나(Ctrl+shift+I)를 하면 된다.**

그리고 이미지에서 분할된 오른쪽에서 맨위 Elements기준 왼쪽끝에 를 누르고 기사제목을 클릭하면 어느 부분을 이용하면 되는지 알 수 있다.

>**코드**

```python
driver = webdriver.Chrome("설치된 저장 경로")
# 파싱을 기다려주는 메소드
driver.implicitly_wait(10) # 간단히 말하자면 10초안에 파싱이 되지않으면 에러가뜨고 종료.

page_count=20 # 페이지 수

def daum_crawling(page_count):
    title=[]# 제목을 담을 리스트
    compy=[]# 언론사를 담을 리스트

    for page in range(1, page_count+1):
        time.sleep(3)# for을 한번돌때마다 3초씩 지연시켜줌
        driver.get("https://search.daum.net/search?w=news&q=%EC%BD%94%EB%A1%9C%EB%82%98%EB%B0%94%EC%9D%B4%EB%9F%AC%EC%8A%A4&DA=PGD&spacing=0&p="+str(page))# 크롤링 할 사이트 호출
        
        # 현재 페이지 저장?로드
        html= driver.page_source

        # BeautifulSoup 파싱방법
        soup= BeautifulSoup(html, 'html.parser')

        # 현재 페이지에 있는 모든 기사 제목 가져오기
        dd = soup.findAll("a", class_="f_link_b")

        # 언론사 가져오기
        co_in = soup.find("div", class_="coll_cont")
        co_info = co_in.select("div.wrap_cont > div > span.f_nb.date")

        link.append(dd)# 기사제목추가
        compy.append(co_info)# 언론사 추가
        
    return link, compy

# 기사 제목만 가져오기
def daum_title(data):
    return [title.get_text() for title in data]

titles, company_name = daum_crawling(page_count)

# 이렇게 해준이유 내가 코르를 이상하게 짜서 그런지는 모르겠지만,
# 이중 리스트로 되어있어서 풀어줬다.
# 예를들어, [["a"], ["b"],[ "c"]] 이거를 --->["a", "b", "c"] 이렇게 만들어줬다
titles = sum(ttles, [])
title = daum_title(titles))

# 언론사도 마찬가지로
comnaa = sum(company_name, [])
comname=[]
for i in comnaa:
    i = i.get_text().strip().split()[2]
    comname.append(i)
print("총 언론사: ",len(comname), "제목 수: ",len(title))

>> 총 언론사:  200 제목 수:  200
#위에 있는 코드 
# i = i.get_text().strip().split()[2]
# 왜 이렇게 해줬냐면 comnaa에 있는 원소들이 
# ["1시간전", "|", "정책브리핑"]이렇게 되어있는데 
# 나는 언론사만 필요하니 2번째 인덱스만 필요하니까
```
간단한 크롤링으로 기사 제목과 언론사를 가져올 수 있다. 

그다음으로 생각 해본게 각 언론사별로 쓴 기사만 모아보면 괜찮을거 같은데 라는생각해 시도 해보았다.

```python
# 각 언론사 끼리 기사 제목 모음 
last_document = {}# 각 언론사별 로 기사를 담기위한 딕셔너리
for i in range(len(comname)):
    
    # 언론사 중복 확인 
    if comname[i] not in last_document.keys():
        last_document[comname[i]]= [title[i]]
    # 만약 언론사가 키 안에 있으면 제목만 넣어주면 된다.
    else:
        last_document[comname[i]].append(title[i])
```
위에 처럼 하면 각언론사 별로 쓴 기사만 볼수있다. 하지만 내가 아직 초보라서 그런지 중간에 가다보면 중복된 기사 제목이 보인다. 

그래서 중복된 기사제목을 지워주기 위해서

```python
# 중복된 기사 제거
for i ,j in last_document.items():
    last_document[i] = list(set(j))# 리스트 안에 set을 해주었다


# 코로나바이러스에 대한 기사를 쓴 언론사들
print(set(comname))
>> 
{'연합뉴스', '문화일보', '건설경제', '동아일보', '내외뉴스통신', '세계비즈', '데일리안', '분당신문', '이데이뉴스닷컴', '정읍시사', '시사저널', 'KBS', '연합인포맥스', '서산인터넷뉴스', '아이뉴스24', '아시아아츠', '글로벌이코노믹', '주간경향', '공감신문', '스포츠동아', '국제신문', '조선일보', '동아사이언스', 'MBN', '이코노미톡뉴스', '정책브리핑', 'BBS', '세계일보', '국민일보', '한국면세뉴스', '팜뉴스', 'SBS', '뉴스한국', '메디칼트리뷴', '전자신문', '한국공보뉴스', '스포츠조선', '지디넷코리아', '글로벌경제신문', '일간투데이', '쿠키뉴스', '스트레이트뉴스', '영천인터넷뉴스', '매일경제', '한국경제', '천지일보', '중앙일보', '스타투데이', '뉴시스', '코메디닷컴', '머니투데이', '청년일보', 'YTN', '한국경제TV', '매일신문', '경남에나뉴스', '일요신문', '오마이뉴스'}
```

솔직히 말해서 각언론사 별로 기사가 모아졌는지 잘 모르겠다 좀 더 공부를해 발전 해야한다.

**이제 데이터 프레임으로 만들어보자**

```python
# 정리하기 위해서 
# 언론사 
company = [name for name,  tit in last_document.items()] 
# 기사 제목
titless = [titles for name, titles in last_document.items()]

# 기사 제목 수
lengh = [len(num) for num in titless] 
# 각언론사별 기사를 몇개 썼는지 
```

```python
import pandas as pd 

df = pd.DataFrame({"Company": company, "Title": titless, "기사 제목수": lengh})

# csv로 저장하기 
df.to_csv("corona.csv", mode="w") 
```
다음에는 이 데이터를 가지고 언론사별 기사를 분석 해볼것이다.


