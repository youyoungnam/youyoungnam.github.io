---
title: Pytorch를 이용해서 MLP설계하기
categories: Pytorch
author_profile: true
---





```python
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch.nn.functional as F
from torchvision import transforms, datasets


if torch.cuda.is_available():
    DEVICE = torch.device("cuda")
else:
    DEVICE = torch.device("cpu")


print("Using Pytorch version: {} Device: {}".format(torch.__version__, DEVICE))
```




```python
Batch_size = 32   # (1)
Epochs = 10       # (1)

train_dataset = datasets.MNIST(root = "../data/MNIST",                        # (2)
                              train = True,
                              download=True,
                              transform= transforms.ToTensor())




test_dataset = datasets.MNIST(root = "../data/MNIST",
                              train = False,
                              transform = transforms.ToTensor())


train_loader = torch.utils.data.DataLoader(dataset = train_dataset,
                                           batch_size = Batch_size,
                                           shuffle= True)
test_loader = torch.utils.data.DataLoader(dataset = test_dataset,
                                          batch_size = Batch_size,
                                          shuffle = False)

```

### (1)설명
**Batch_size는 학습할 때 필요한 데이터 개수 단위 즉, 32개씩 학습 예를들어 32개의 데이터를 이용해 학습을 한번하면 
다음에도 역시 32개의 데이터로 학습을 진행**

**좀  더 자세한 예를들자면 총 1만개의 데이터가 있다고 가정하자. 
그때 batch_size가 1000이면 총 iteration(도는 횟수)가 10번이다 1000*10 == 10000 1 Epoch가 1000 * 10 이다. 즉, 1 Epoch당 10회의 iteration가 발생 **




### (2)설명
**파라미터 하나씩 설명하자면**
  - root는 데이터가 저장될 장소를 지정 ../는 상위폴더 즉 여기서는 data 폴더 내 MNIST 폴더에 저장하는 내용
  - train은 MLP 모델을 학습하기 위해 이용하는 학습용 데이터인지 검증용 데이터인지 지정해주기 위해 사용
  - train데이터니 train = True를 줘야함 
  - trainsform은 데이터가 사람의 손글씨 이미지 이기때문에 이미지 데이터의 기본적인 전처리를 동시에 가능 
  - 여기에 torch의 모듈을 활용해 toTensor 메서드를 이용해 tensor형태로 바꿔줌 또한 한 픽셀은 0~255 범위의 스카라 값으로 구성 돼어있음 이를 0~1 값으로 정규화 과정도 포함 
  - MLP모델이 포함된 인공 신경망 모델은 Input 데이터 값의 크기가 커질수록 불안정하거나 과적합되는 방향으로 하습이 진행 될 우려가 있음 그렇기에 정규화진행
  - 이미지 데이터 1개는 각각을 이용해 MLP 모델을 학습시키는 것이 아니라 이미지 데이터를 batch_size만큼 즉, 32개씩 묶에 1개의 mini-batch를 구성하는 것을 DataLoader함수에서 진행 


**Batch_size Mini-batch는 1개 단위를 구성하는 데이터의 개수를 지정 
shuffle 은 데이터의 순서를 섞고자 할 때 이용 MLP모델이 학습을 진행할 때 Label 정보의 순서를 암기해 학습을 진행 할 수 있음
즉, 특정 Label에 매칭된 이미지 데이터의 특정을 보고 학습하는 것이 아니라 특정 이미지 데이터에 매칭된 Label값만을 집중적으로 학습을 진행하는 상황 발생 즉, 잘못된 방향으로 학습하는걸 방지하고자 데이터 순서를 섞는 과정을 진행**
```python
for x_train, y_train in train_loader:
    print("x_train size: {} type: {}".format(x_train.size(), x_train.type())) # (3)
    print("y_train size: {} type: {}".format(y_train.size(), y_train.type()))
    break

print("="*50)
print("모델 설계")


class Net(nn.Module):                                                        #(4)
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 10)

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        x = self.fc1(x)
        x = F.sigmoid(x)
        x = self.fc2(x)
        x = F.sigmoid(x)
        x = self.fc3(x)
        x = F.log_softmax(x, dim = 1)
        return x

model = Net().to(DEVICE)                                                   #(5)
optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)
criterion = nn.CrossEntropyLoss()

print(model)
```
### (3)설명
**x_train 32개의 이미지 데이터가 1개의 mini-batch를 구성하고 있고 가로 28개 세로 28의 픽셀로 구성 채널은 1 그레이** 
**y_train은 32개의 이미지 데이터 각각의 Label값이 1개씩 존재하기 때문에 32개의 값을 가지고 있음** 

**정리하자면 x_train은 32개의 이미지데이터가 1개의 mini-batch를 구성하고 있고 가로 28 세로 28의 픽셀로 구성 돼 있으며 
채널이 1이므로 그레이 스케일로 이뤄진 다시말해 흑백으로 이뤄진 이미지 데이터라는걸 확인 
y_train은 32개의 이미지 데이터 Label값이 1개씩 존재하기 때문에 32개의 값을 가지고 있다는 것을 확인**



### (4)설명

**torch 모듈을 이용해 본격적으로 MLP을 설계하는 단계
Pytorch Module 내에 딥러닝 모델 관련 기본 함수를 포함하고 있는 nn.Module 클래스를 상속 받는 Net 클래스를 정의 
nn.Module 클래스를 상속 받았을 때 nn.Module 클래스가 이용할 수 있는 함수를 그대로 이용할 수 있기 떄문에 새로운 딥러닝 모델을 설계할 때 자주 이용됨**


**Net 클래스의 인스턴스를 생성해씅ㄹ 때 지니게 되는 성질을 정의해주는 메서드 이다.
nn.Module내에 쓰이는 메서드를 상속받아 이용
첫번째 Fully Connected Layer를 정의 MNIST 데이터를 Input으로 사용하기 위해 28*28*1(가로 세로 채널수) 크기의 노드 수를 설정한 후 두번째 Fully Connected Layer의 노드수를 512개로 설정할 것이기 때문에 output의 노드 수는 512개로 설정
두번째 Fully Connected Layer를 정의 첫번째 FLC의 output크기가 512크기의 벡터 값을 input으로 사용하기위해 512로 설정하고
세 번째 FLC의 노드수를 256으로 설정할것이기 때문에 output의 노드 수를 256개로 설정**



**세 번째 FLC를 정의 두번째 FLC의 output크기가 256 크기의 벡터값을 사용하기에 노드 수를 256개 output으로 사용하기 위한 노드 수를 10개로 설정  0~ 9까지 총 10가지 클래스를 표현하기 위한 Label값은 원 핫 인코딩으로 표현 됨
MLP 모델의 output 값과 Loss를 계산하려면 이에 맞는 크기의 벡터를 계산해야함 따라서 output node의 수는 10개로 정의
Net 클래스를 이용해 설계한 MLP 모델의 Forward Propagation을 정의 즉, 설계한 MLP모델에 데이터를 입력했을 때 output을 계산하기까지의 과정을 나열한 것을 의미**



**MLP모델은 1차원의 벡터 값으로 받을 수 있다 하지만 mnist의 이미지 데이터 크기는 28*28인 2차원 데이터 따라서 2차원의 데이터를 1차원의 데이터로 변환하려면 view 메서드를 이용해 784 크기의 1차원의 데이터로 변환해 진행해야 합니다. 이를 2차원의 데이터를 1차원으로 펼친다라고 표현하며 Flatten이라고 한다.**


**__init__ method를 이용해 정의한 첫 번째 FLC에 1차원으로 펼친 이미지 데이터를 통과 시킨다 
pytorch module중 인공 신경망 NN(Neural Network) 설계에 유용한 함수를 모아 놓은 torch.nn.functional 내에 정의된 비선형 함수인 sigmoid()을 이용해 두번째 FLC의 input으로 계산**



**__init__() method를 이용해 정의한 두 번째 Fully Connected Layer에 sigmoid 함수를 이용해 계산된 결괏값을 통과시킴
다시 세 번째 FLC에 에서 sigmoid를 이용해 계산된 결과값을 통과시킴**



**pytorch.Module 중 인공신경망 설계에 유용한 함수를 모아 놓은 torch.nn.functional 내의 log.softmax()를 이용해 최종 output을 계산 이때 0~9까지 총 10가지 경우의 수중 하나로 분류하는 일을 수행하기 때문에 softmax를 이용해 확률 값을 계산한다.
그리고 일반적인 softmax가 아니라 log_softmax인 이유는 MLP모델이 Back Propagation알고리즘을 이용해 학습을 진행할 때 
Loss 값에 대한 Gradient 값을 좀 더 원활하게 계산할 수 있기 때문이다 Log함수 그래프의 기울기가 부드럽게 변화하는 것을 상상해 보면 직관적으로 이해할 수 있다.**


### (5)설명

**Back Propagation을 이용해 파라미터를 업데이트할 때 이용하는 Otimizer를 정의
이 예제에서는 Stochastic Gradient Descent(SGD) 알고리즘을 이용하며 파라미터를 업데이트할 때 반영될 Learning_rate=0.01,
Otimizer의 관성을 나타내는 momentum 0.5로 설정 
MLP 모델의 output값과 계산될 Label 값은 class를 표현하는 원-핫 인코딩 값이다.
MLO모델의 output 값과 원-핫 인코딩 값과의 Loss는 CrossEntropy를 이용해 계산하기 위해 criterion은 nn.CrossEntropyLoss()로 설정**



```python

def train(model, train_loader, optimizer, log_interval):                                  #(6)
    model.train()
    for batch_idx, (image, label) in enumerate(train_loader):
        image = image.to(DEVICE)
        label = label.to(DEVICE)
        optimizer.zero_grad()
        output = model(image)
        loss = criterion(output, label)
        loss.backward()
        optimizer.step()

        if batch_idx % log_interval == 0:
            print("Train Epoch: {} [{}/{} ({:.0f}%)]\tTrain Loss: {:.6f}".format(
                Epochs, batch_idx * len(image),
                len(train_loader.dataset), 100. * batch_idx / len(train_loader),
                loss.item()))



def evaluate(model, test_loader):                                                 #(7)
    model.eval()
    test_loss = 0
    correct = 0

    with torch.no_grad():
        for image, label in test_loader:
            image = image.to(DEVICE)
            label = label.to(DEVICE)
            output = model(image)
            test_loss += criterion(output, label).item()
            prediction = output.max(1, keepdim=True)[1]
            correct += prediction.eq(label.view_as(prediction)).sum().item()

    test_loss /= len(test_loader.dataset)
    test_accuracy = 100. * correct / len(test_loader.dataset)
    return test_loss, test_accuracy


for Epoch in range(1, Epochs+1):                                        #(8)
    train(model, train_loader, optimizer, log_interval=200)
    test_loss, test_accuracy = evaluate(model, test_loader)
    print("\n[EPOCH: {}], \tTest Loss: {:4f}, \tTest Accuracy: {:1f} %\n".format(Epoch, test_loss, test_accuracy))

```

### (6)설명

**MLP 모델을 설계했으므로 기존에 정의한 이미지 데이터와 레이블 데이터를 이용해 MLP 모델을 학습하는 train함수를 정의
기존에 정의한 train_loader에는 학습에 이용되는 이미지 데이터와 레이블 데이터가 Mini-Batch단위로 묶여 저장돼 있다.
해당 train_loader 내에 mini-batch 단위로 저장된 데이터를 순서대로 이용해 mlp모형을 학습**

**Mini-batch 내에 있는 이미지 데이터와 매칭된 레이블 데이터도 기존에 정의한 장비에 할당한다.
기존에 정의한 장비에 이미지 데이터와 레이블 데이터를 할당할 경우 과거에 이용한 mini-batch 내에 있는 이미지 데이터와 레이블 데이터를 바탕으로 계산된 Loss의 Gradient 값이 optimizer에 할당돼 있으므로 optimizer의 gradient를 초기화 한다.
장비에 할당한 이미지 데이터를 mlp모델의 input으로 이용해 output을 계산**

**계산된 output과 장비에 할당된 레이블 데이터를 기존에 정의한 CrossEntropy를 이용해 Loss값을 계산한다
loss값을 계산한 결과를 바탕으로 back Propagation을 통해 계산된 gradient 값을 각 파라미터에 할당한다.
각 파라미터에 할당된 Gradient 값을 이용해 파라미터 값을 업데이트한다** 




### (7)설명

**MLP 모델 학습 과정 또는 학습이 완료된 상태에서 MLP모델의 성능을 평가하기 위해 evaluate함수를 정의
학습과정 또는 학습이 완료된 MLP모델을 학습 상태가 아닌 평가 상태로 지정한다
기존에 정의한 test_loader 내의 데이터를 이용해 Loss 값을 계산하기위해 test_loss를 0으로 임시 설정**



**MLP 모델을 평가하는 단계에서는 Gradient를 통해 파라미터 값이 업데이트 되는 현상을 방지하기 위해 torch.no_grad()
메서드를 이용해 Gradient의 흐름을 억제한다.
기존에 정의한 test_loader 내의 데이터도 train_loader와 동일하게 mini-batch 단위로 저장돼 있다 mini-batch내에 있는 이미지 데이터와 레이블 데이터에 반복문을 이용해 차례대로 접근한다 
mini-batch내에 있는 이미지 데이터를 이용해 MLP 모델을 검증하기 위해 기존에 정의한 장비에 할당 한다.**




**mini-batch 내에 있는 이미지 데이터와 매칭된 레이블 데이터도 기존에 정의한 장비에 할당한다.
장비에 할당한 이미지 데이터를 MLP모델의 input으로 output을 계산한다 
계산된 output과 장비에 할당된 레이블 데이터를 기존에 정의한 CrossEntropy를 이용해 Loss값을 계산한 결과값을 test_loss에 더해 업데이트 한다.
MLP 모델의 output 값은 크기가 10인 벡터 값이다.
계산된 벡터 값 내 가장 큰 값인 위치에 대해 해당 위치에 대응하는 클래스로 예측했다고 판단**


**MLP 모델이 최종으로 예측한 클래스 값과 실제 레이블이 의미하는 클래스가 맞으면 correct에 더해 올바르게 예측한 횟수를 저장
현재까지 계산된 test_loss의 값을 test_loader 내에 존재하는 moni-batch 개수만큼 나눠 평균 Loss값으로 계싼한다.
test_loader 데이터 중 얼마나 맞췄는지를 계산해 정확도를 계싼 
계산된 test_loss 값과 test_accuray값을 반환한다.**




### (8)설명


**train 함수와 evaluate 함수를 올바르게 정의했다면 정의한 함수를 이용해 MLP 모델을 학습시키거나 검증해보는 과정을 진행
전체 데이터를 이용해 학습하는 횟수를 의미하는 Epoch을 10으로 설정 했기 때문에 10번의 학습을 진행하고 학습 과정 속에서 업데이트한 파라미터 값을 바탕으로 MLP모델의 Output이 변화하며
각 Iteration, Epoch당 Loss값이 출력되도록 설정**


**정의한 train 함수를 실행 model은 기존에 정의한 MLP모델, train_loader는 학습 데이터, optimizer는 SGD , log_interval은 학습이 진행되면서 mini-batch의 index를 이용해 과정을 모니터링 할 수 있도록 출력하는 것을 의미**


**각 Epoch별로 출력이 되는 Loss값과 accuracy값을 계산한다. 학습이 완료됐을 때 test_loader 내에 존재하는 데이터의 약 90% 수준의 정확도를 나타내는 것을 확인**



<img src="/assets/images/instal.PNG">