---
title: Pytorch를 이용해서 Droupout 적용해보기
categories: Pytorch
author_profile: true
---




```python
Batch_size = 32
Epochs = 20

train_dataset = datasets.MNIST(root = "../data/MNIST",
                              train = True,
                              download=True,
                              transform= transforms.ToTensor())




test_dataset = datasets.MNIST(root = "../data/MNIST",
                              train = False,
                              transform = transforms.ToTensor())


train_loader = torch.utils.data.DataLoader(dataset = train_dataset,
                                           batch_size = Batch_size,
                                           shuffle= True)
test_loader = torch.utils.data.DataLoader(dataset = test_dataset,
                                          batch_size = Batch_size,
                                          shuffle = False)


for x_train, y_train in train_loader:
    print("x_train size: {} type: {}".format(x_train.size(), x_train.type()))
    print("y_train size: {} type: {}".format(y_train.size(), y_train.type()))
    break

```


**Dropout을 Pytorch를 이용해 적용하는 것은 매우 간단하다 지난번에 0~9까지 사람이 그린 손글씨 데이터인 MNIST를 활용해 10개의 클래스로 분류한 문제를 했었다. 그 코드에 Dropout을 어떻게 적용하는지 보자.
지난번에는 실습할 때는 nn.Module을 이용해 MLP을 설계했다. def.__init__(self) 메서드를 이용해 MLP를 설계하는 데 필요한 요소를 정의 했고 def foward(self, x)메서드를 이용해 MLP에 데이터를 입력했을 때 output으로 계산되는 과정을 정의했다.**


**MLP를 설계하는 이 부분에 Dropout을 적용하자고한다. 파이토치를 이용해 Droupout을 적용하는 것은 매우 간단하다. 우선 해당 Layer에 몇펴센트의 노드에 대해 가중값을 계산하지 않을 것인지 명시적으로 정해줘야함
이 예제에서는 30%정도의 노드들은 계싼하지 않는 것으로 하겠다. 그다음, 설계한 MLP구조 내의 어느 부분에 Droupout을 적용할 것인지 명시해야함 이 예제에서는 비선형 함수의 output에 적용하기 위해 x = F.sigmoid(x)의 결괏값과 x = F.sigoid(x)의 결과값에 Droupout을 적용하고자한다. 밑에 코드를 보자.**



```python
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 10)
        # Drouput을 정의
        self.dropout_prob = 0.5                             # (1)

    def forward(self, x):
        x = x.view(-1, 28 * 28)
        x = self.fc1(x)
        x = F.sigmoid(x)
        x = F.dropout(x, training=self.training, p = self.dropout_prob)            #(2)
        x = self.fc2(x)
        x = F.sigmoid(x)
        x = F.dropout(x, training=self.training, p = self.dropout_prob)
        x = self.fc3(x)
        x = F.log_softmax(x, dim = 1)
        return x
```

## (1)설명
**몇퍼센트의 노드에 대해 가중값을 계산하지 않을 것인지를 명시해주는 부분이다. 이 예제에서는 50%의 노드에 대해 가중값을 게산하지 않기 위해 0.5으로 정의했다.**



## (2)설명
**각 sigmoid함수의 결과값에 대해 dropout을 적용하는 부분이다 계산되는 과정속에 있는 x값에 적용하며 p값은 몇퍼센트의 노드에 대해 계산하지 않을 것인지를 조정하는 요소이다 여기서 training = self.training부분이 있는데 이 부분은 학습 상태일 때와 검증 상태에 따라 다르게 적용되기 위해 존재하는 파라미터이다. 
dropout은 학습과정 속에서 랜덤으로 노드를 선택해 가중값이 업데이트되지 않도록 조정하지만 평가 과정 속에서는 모든 노드를 이용해 output을 계산하기 때문에 학습 상태와 검증 상태에서 다르게 적용돼어야 한다.
이를 반영하기 위한 파라미터 값을  model.train()으로 명시할 때 self.training = True, model.eval() self.training = False로 적용됨**



```python
model = Net().to(DEVICE)
optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)
criterion = nn.CrossEntropyLoss()

print(model)



def train(model, train_loader, optimizer, log_interval):
    model.train()
    for batch_idx, (image, label) in enumerate(train_loader):
        image = image.to(DEVICE)
        label = label.to(DEVICE)
        optimizer.zero_grad()
        output = model(image)
        loss = criterion(output, label)
        loss.backward()
        optimizer.step()

        if batch_idx % log_interval == 0:
            print("Train Epoch: {} [{}/{} ({:.0f}%)]\tTrain Loss: {:.6f}".format(
                Epochs, batch_idx * len(image),
                len(train_loader.dataset), 100. * batch_idx / len(train_loader),
                loss.item()))


def evaluate(model, test_loader):
    model.eval()
    test_loss = 0
    correct = 0

    with torch.no_grad():
        for image, label in test_loader:
            image = image.to(DEVICE)
            label = label.to(DEVICE)
            output = model(image)
            test_loss += criterion(output, label).item()
            prediction = output.max(1, keepdim=True)[1]
            correct += prediction.eq(label.view_as(prediction)).sum().item()

    test_loss /= len(test_loader.dataset)
    test_accuracy = 100. * correct / len(test_loader.dataset)
    return test_loss, test_accuracy



for Epoch in range(1, Epochs+1):
    train(model, train_loader, optimizer, log_interval=200)
    test_loss, test_accuracy = evaluate(model, test_loader)
    print("\n[EPOCH: {}], \tTest Loss: {:4f}, \tTest Accuracy: {:1f} %\n".format(Epoch, test_loss, test_accuracy))

```
## Sigmoid와 Droupout으로 학습했을 때 결과 
<img src = "/assets/images/sigmoid_dropout.PNG">


## Sigmoid와 Droupout으로 학습을 진행하고 학습 횟수를 늘렸을 때 결과
<img src = "/assets/images/sigmoid_dropout_iteration_up.PNG">



## relu 와 Droupout으로 학습을 진행했을 때 결과 
<img src = "/assets/images/relu_droupout.PNG">
## 결과
이론상 dropout을 적용했을 때 일반화가 강해져 Test Accuracy가 높아지는 결과가 기대되지만, 이는 학습 데이터셋과 검증 데이터셋의 피처 및 레이블의 분포 간 많은 차이가 있을 때 유효하게 작용된다. 
MNIST는 학습 데이터와 검증데이터 간 많은 차이가 발생하지 않기 때문에 오히려 성능이 조금 하락 할 수 있다.하지만, Epoch을 조금 올리면 성능이 좋아지는 경향이있음 **Dropout은 보통 Relu 비선형함수와 잘 어울린다.**

**확실히 Sigmoid 비선형함수를 사용하는것 보다 relu와 droupout을 사용했을 때 성능이 향상되는걸 볼 수 있다.**
