---
title: TF-IDF(Term Frequency-Inverse Document Frequency)
categories: NLP
author_profile: true
---
## TF-IDF(Term Frequency-Inverse Document Frequency)

TF-IDF는 DTM에 있는 각 단어에 대한 중요도를(가중치를) 
부여하고, TF-IDF를 사용하면 DTM을 사용하는것 보다 더 많은 정보를 고려하여 문서들을 비교 할 수 있다.

>**※주의:** 항상 TF_IDF가 DTM보다 성능이 뛰어나다고 할 수 없다.

TF-IDF(Term Frequency-Inverse Document Frequency)는 단어의 빈도와 역 문서 빈도를 사용하여 DTM내의 각 단어들마다 중요한 정도를 가중치를 주는 방법이다.

**사용방법**
 - 1 먼저 DTM을 만든다.
 - 2 TF-IDF 가중치를 부여한다.

**그렇다면 TF_IDF는 어디에 사용할 수 있을까?**
 - 중요도를 정하는 작업
 - 문서 내에서 특정단어의 중요도를 구하는 작업


TF-IDF는 TF와 IDF를 곱한 값을 의미한다. 이것을 식으로 표현 해보겠다.

**문서를 d, 단어를 t, 문서의 총 개수를 n**

**tf(d,t): 특정 문서 d에서의 특정단어 t의 등장 횟수**
*위 수식만 보면 이게 무슨말이지? 할 수 있는데 이것은 DTM에 각 문서에서 단어의 등장 빈도를 나타낸값이다.*

다시 한번 보자면

>문서1: 먹고 싶은 사과 
|과일이|길고|노란|먹고|바나나|사과|싶은|저는|좋아요|
   0    0    0    1    0     1   1    0    0

**df(t): 특정 단어 t가 등장한 문서의 수**

어떤 문서에서 어떤 단어가 몇번 나왔는지는 중요하지 않고, 어떤 단어가 등장한 문서의 수 만 중요하다. 

**예를들어, 문서1에서 치킨이 2백번 문서2에서 3백번 나왔다고 생각해보자. 그러나 200번, 300번 나온게 중요하지않고, 문서1, 문서2에서  나왔다는게 중요하다.**



**idf(d,t): dt(t)에 반비례하는 수**

$idf(d,t) = log(n/1+d(t))$

**분모에 1를 더하는 이유가 무엇일까? 특정단어가 전체 문서에서 등장하지 않을 경우 분모가 0이 되는걸 방지 해야하기 때문이다.**


**결국**, TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에만 자주 등장하는 단어는 중요도가 높다고 판단한다.


### 실습


```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    'you know I want your love',# 문서1
    'I like you',# 문서2
    'what should I do ', #문서3    
]
vector = CountVectorizer()

print(vector.fit_transform(corpus).toarray())
print(vector.vocabulary_)

>>[[0 1 0 1 0 1 0 1 1] #문서1
 [0 0 1 0 0 0 0 1 0] # 문서2
 [1 0 0 0 1 0 1 0 0]]# 문서3

>>{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}
```

##### TF-IDF

```python
from sklearn.feature_extraction.text import TfidfVectorizer
corpus = [
    'you know I want your love',
    'I like you',
    'what should I do ',    
]
tfidfv=TfidfVectorizer().fit(corpus)
print(tfidfv.transfom(corpus).toarray())
print(tfidfv.vocabulary_)

>>[[0.         0.46735098 0.         0.46735098 0.         0.46735098 0.         0.35543247 0.46735098]
 [0.         0.         0.79596054 0.         0.         0.         0.         0.60534851 0.        ]
 [0.57735027 0.         0.         0.         0.57735027 0.         0.57735027 0.         0.        ]] 
 ## DTM에서 각 단어의 가중치가 부여된걸 볼 수 있다.

>>{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}

```

**위에 내용은 [딥러닝을 이용한 자연어처리 입문](https://wikidocs.net/book/2155) 여기서 공부한 내용을 쓴것이다.**
