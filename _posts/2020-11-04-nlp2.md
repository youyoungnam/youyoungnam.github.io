---
title: Pretrained embedding을 활용해서 영화리뷰 
categories: kaggle
author_profile: true
---



### Kaggle Sentiment Analysis on Movie Reciews 데이터 사용



**데이터로드** 
```python
train = pd.read_table("/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip")
test = pd.read_table("/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip")

# 데이터 확인
display(train, test)
```
<img src="c1.png">


**영화 리뷰 하나 가져와보기**
```python
print(train["text"][0])
>>
'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander,
some of which occasionally amuses but none of which amounts to much of a story .'

```
**무슨말인지는 잘 모르겠지만 good이라는 단어가 있어서 좋은 리뷰인거 같지만 정답값을보면 긍정 리뷰가 아니다.**

**train, test 데이터 셋 합치기 합치는 이유는 한번에 전처리를 해주고 학습하기전에 분리 해주면 된다.**

```python
all_data = pd.concat([train,test])
all_data
```
<img src= "c2.png">

**이제 text 전처리를 해보자. 일단 나는 Pre trained embedding을 사용해서 모델성능이 얼마나 올라가는지를 보기위해서 간단한 전처리만 진행**

```python
from keras.preprocessing.text import Tokenizer

tk = Tokenizer()
tk.fit_on_texts(all_data["Phrase"])

# word_index을 하면 가장 많이 사용하는 단어를 뽑아준다. 
tk.word_index
>>
 {'the': 1,
 'a': 2,
 'of': 3,
 'and': 4,
 'to': 5,
 "'s": 6,
 'in': 7,
 'is': 8,
 'that': 9,
 'it': 10,
 'as': 11,
 'with': 12,
 'for': 13,
 'its': 14,
 'film': 15,
 'an': 16,
 'movie': 17,
 'this': 18,
 'but': 19,
 'be': 20,
 'on': 21,
 'you': 22,
 'by': 23,
 "n't": 24,
 'more': 25,
 'his': 26,
 'not': 27,
 'one': 28,
 'than': 29,
 'about': 30,
 'at': 31,
 'from': 32,
 'or': 33,
 'all': 34,
 'like': 35,
 'are': 36,
 'have': 37,
 'has': 38,
 'so': 39,
 "'": 40,
 'out': 41,
 'story': 42,
 'who': 43,
 'rrb': 44,
 'up': 45,
 'too': 46,
 'good': 47,
 'most': 48,
 'into': 49,
 'lrb': 50,
 'time': 51,
 'much': 52,
 'what': 53,
 'if': 54,
 'characters': 55,
 'no': 56,
 'comedy': 57,
 'their': 58,
 'just': 59,
 'i': 60,
 'some': 61,
 'can': 62,
 'even': 63,
 'life': 64,
 'your': 65,
 'little': 66,
 'does': 67,
 "''": 68,
 'way': 69,
 'well': 70,
 'will': 71,
 'make': 72,
 'been': 73,
 'funny': 74,
 'only': 75,
 'very': 76,
 'he': 77,
 'do': 78,
...
}

# a가 몇번했는지 보고싶으면
tk.word_counts['a']
>> 50432
# 텍스트를 시퀀스로 바꿔주는 작업이다.
all_texts = tk.texts_to_sequences(all_data["Phrase"])
all_texts
>>
[[2,
  315,
  3,
  16573,
  7660,
  1,
  8313,
  9,
  53,
  8,
  47,
  13,
  1,
  3940,
  8,
  187,
  47,
  13,
  1,
  13024,
  61,
  3,
  89,
  592,
  12156,
  19,
  617,
  3,
  89,
  2810,
  5,
  52,
  3,
  2,
  42],
 [2, 315, 3, 16573, 7660, 1, 8313, 9, 53, 8, 47, 13, 1, 3940],
 [2, 315],
 [2],
 [315],
 [3, 16573, 7660, 1, 8313, 9, 53, 8, 47, 13, 1, 3940],
 [3],
 [16573, 7660, 1, 8313, 9, 53, 8, 47, 13, 1, 3940],
 [16573],
 [7660, 1, 8313, 9, 53, 8, 47, 13, 1, 3940],
 [7660, 1, 8313],
 [7660],
 [1, 8313],
 [1],
 [8313],
 [9, 53, 8, 47, 13, 1, 3940],
 [9],
 [53, 8, 47, 13, 1, 3940],
 [53],
 [8, 47, 13, 1, 3940],
 [8],
 [47, 13, 1, 3940],
 [47],
 [13, 1, 3940],
 [13],
 [1, 3940],
 [3940],
 [8,
  187,
  47,
  13,
  1,
  13024,
  61,
  3,
  89,
  592,
  12156,
  19,
  617,
  3,
  89,
  2810,
  5,
  52,
  3,
  2,
  42]
  ...

# tk.word_index
# 어떤 단어가 어떤 인덱스를 가지고 있는지 보는 방법이다 
tk.word_index
```


**근데 여기까지는 좋다 모델에 학습하려면 차원이 같아야한다. 즉, 각 문장 길이가 제각각 이면 안된다는 소리이다. 그래서 문장길이를 맞춰주기 위해서 padding작업을 해줘야 한다.**

```python
from keras.preprocessing.sequence import pad_sequences


# 학습으 빠르게 하고 싶으면 max_len을 주면됨  
all_pad = pad_sequences(all_texts)
all_pad
>>
array([[   0,    0,    0, ...,    3,    2,   42],
       [   0,    0,    0, ...,   13,    1, 3940],
       [   0,    0,    0, ...,    0,    2,  315],
       ...,
       [   0,    0,    0, ...,    2,  118, 4456],
       [   0,    0,    0, ...,    2,  118, 4456],
       [   0,    0,    0, ...,    0,  343, 1623]], dtype=int32)
```


**이제 Pretrain embedding을 가져와서 학습을 할 것이다 일단. 내가 사용하는 prtrain embedding은 Facebook에서 wikipedia의 데이터를 학습한 embedding이다(영어 뿐만아니라 한국어도 있다.).**


```python
def load_embedding(file_path):
    embedding = {}
    with open(file_path) as  f:
        for i in f:
            value = i.strip().split()
            words = value[0]
            vector = np.asarray(value[1:], dtype = "float32") 
            embedding[words] = vector
    return embedding 

# 미리 다운받은 embedding
embedding = load_embedding("/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec")
embedding
>>{'2000000': array([ 2.0600e-02,  1.9530e-01, -9.0400e-02, -3.5390e-01, -6.2700e-02,
        -1.4600e-02, -1.3150e-01,  5.8600e-02,  5.9930e-01,  6.3100e-02,
        -9.3200e-02,  7.1720e-01, -3.4950e-01, -6.1100e-02, -3.0790e-01,
         3.6940e-01, -2.5880e-01, -3.0210e-01, -1.2800e-02,  3.2680e-01,
         6.9900e-02,  8.9400e-02, -1.1910e-01, -1.1900e-01, -1.2200e-01,
        -4.5400e-02, -2.5100e-02, -1.7630e-01,  7.6370e-01, -8.4900e-02,
        -4.1930e-01,  3.0050e-01, -9.8500e-02,  1.6770e-01, -2.0570e-01,
        -1.8150e-01, -2.7360e-01,  3.7540e-01, -6.6400e-02,  1.7150e-01,
         4.6900e-01,  3.0410e-01,  1.8830e-01,  1.0950e-01,  4.7590e-01,
         1.1540e-01, -2.9670e-01, -3.4600e-02,  1.4270e-01,  4.1870e-01,
         1.2770e-01,  1.7650e-01,  7.1670e-01,  5.0060e-01, -5.6000e-03,
        -1.0700e-01, -3.6280e-01,  5.4290e-01,  1.0060e-01, -4.4410e-01,
        -6.2800e-02, -3.6000e-03,  8.1900e-02, -4.5830e-01,  8.3400e-02,
        -9.4100e-02,  7.5100e-02,  3.7370e-01, -1.0820e-01, -1.5300e-02,
        -1.3900e-02, -5.9000e-02, -1.1600e-02, -8.0970e-01, -1.6890e-01,
         6.1260e-01,  1.5200e-02, -1.7120e-01, -2.8400e-01,  3.7140e-01,
         9.0600e-02,  2.4040e-01, -1.2440e-01, -3.8910e-01,  4.1730e-01,
        -9.1600e-02,  6.1100e-02,  5.9410e-01, -6.5990e-01, -1.0140e-01,
         2.5740e-01,  5.5680e-01, -1.1930e-01, -3.3940e-01, -3.7260e-01,
         2.3520e-01,  8.0200e-02, -2.4910e-01,  9.8300e-02, -1.0260e-01,
        -3.4620e-01,  2.9500e-02, -1.8830e-01,  1.1920e-01, -2.3710e-01,
         1.2340e-01, -1.1770e-01, -6.8190e-01,  4.2390e-01,  2.9710e-01,
         3.3000e-03,  7.3810e-01,  2.5250e-01, -2.3500e-02, -1.1836e+00,
         2.6000e-01,  3.4200e-02, -3.9810e-01, -1.6400e-02,  5.1290e-01,
        -3.8360e-01,  1.9830e-01, -1.1460e-01,  6.3550e-01, -2.4790e-01,
        ........}
```

**이제 내가 전처리한 데이터에 pre_train_embbedding값으로 바꿔주자**


```python
def filter_embedding(embedding, word_index, vocab_size, dim):
    embedding_matrix = np.zeros([vocab_size, dim])
    
    for word, i in word_index.items():
        vector = embedding.get(word)
        if vector is not None:
            embedding_matrix[i] = vector 
    return embedding_matrix

embedding_matrix = filter_embedding(embedding, tk.word_index, len(tk.word_index)+1, 300)
embedding_matrix

```


**이제 train, test 데이터를 나눠주고 학습을 하면된다**


```python


from keras import Sequential
from keras.layers import *

pad_train = all_pad[:len(train)]
pad_test = all_pad[len(train):]


# 학습하는 부분
model = Sequential()

# pre_train_embedding layer
model.add(Embedding(len(tk.word_index)+1, 300, weights= [embedding_matrix], trainable= False ,input_length=52))
# 문맥파악 모델을 넣어줌 
model.add(LSTM(32))
# model.add(Flatten())
model.add(Dense(5, activation= "softmax"))
model.compile(optimizer= "adam", loss= "sparse_categorical_crossentropy", metrics = ["acc"])
model.fit(pad_train, train["Sentiment"], epochs=5, batch_size=512)
result = model.predict(pad_test)
>>
Epoch 1/5
305/305 [==============================] - 5s 15ms/step - loss: 1.0197 - acc: 0.5906
Epoch 2/5
305/305 [==============================] - 5s 15ms/step - loss: 0.8763 - acc: 0.6346
Epoch 3/5
305/305 [==============================] - 5s 15ms/step - loss: 0.8501 - acc: 0.6461
Epoch 4/5
305/305 [==============================] - 4s 14ms/step - loss: 0.8311 - acc: 0.6534
Epoch 5/5
305/305 [==============================] - 4s 15ms/step - loss: 0.8153 - acc: 0.6613
```

## 제출

```python
sub = pd.read_csv("/kaggle/input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv")
sub

result = result.argmax(1)

train.values
>>
array([[1, 1,
        'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .',
        1],
       [2, 1,
        'A series of escapades demonstrating the adage that what is good for the goose',
        2],
       [3, 1, 'A series', 2],
       ...,
       [156058, 8544, 'avuncular chortles', 3],
       [156059, 8544, 'avuncular', 2],
       [156060, 8544, 'chortles', 2]], dtype=object)


mapping = {phrase: sentiment for _,_,phrase, sentiment in train.values}
mapping
>>
{'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .': 1,
 'A series of escapades demonstrating the adage that what is good for the goose': 2,
 'A series': 2,
 'A': 2,
 'series': 2,
 'of escapades demonstrating the adage that what is good for the goose': 2,
 'of': 2,
 'escapades demonstrating the adage that what is good for the goose': 2,
 'escapades': 2,
 'demonstrating the adage that what is good for the goose': 2,
 'demonstrating the adage': 2,
 'demonstrating': 2,
 'the adage': 2,
 'the': 2,
 'adage': 2,
 'that what is good for the goose': 2,
 'that': 2,
 'what is good for the goose': 2,
 'what': 2,
 'is good for the goose': 2
 ....
 }
 for i, j in enumerate(test["Phrase"]):
    if j in mapping:
        result[i] = mapping[j]

# 가장 높은 값을 가진 값을 가져옴 
sub["Sentiment"] = result
sub

```
<img src="c3.png">


### 모델 성능 보기

<img src="c4.png">

**진짜 간단한 전처리와 pr_trained_embedding과 간단한 모델을 만들고 적은 학습 횟수로 좋은 성능이 나오는걸 볼 수 있다.  밑에 사진을 보면 등수는 10위 안에 드는걸 볼 수 있다.**

<img src="c6.png">