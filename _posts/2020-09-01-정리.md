---
title: IG(information Gain) & Entropy 정리
categories: deeplearning_study
author_profile: true
---


**일단 들어가기전에 머신러닝 할때 CSV파일을 불러와서 데이터를 썼다 이 CSV파일을 제대로 이해가 필요하다. 일단, CSV파일을 데이터셋, 테이블(데이터베이스), CSV라고 부른다.
데이터셋의 객체는 행(데이터베이스), 사례(통계학)이라고 부른다. 객체의 열(속성) 독립변수, 예측자, 설명변수라고 부른다 예측변수는 타깃변수 라벨, 종속변수 우리가 예측해야 할 변수이다.**


Feature Extraction
   - 데이터로 표현되는 객체의 정보를 전달하는 중요한 변수인 속성을(attribute)을 찾아내거나 선택하는일
   - 불확실성을 감소시키는 성질, 단서를 제공하는 변수를 확인하는 작업 예측하려는 타겟과 연관된 불확실성을 줄여주는 속성 추출


IG(Information Gain) & Entropy
  - Entropy
    - 집합이 얼마나 무질서한지 보여줌(순수함, 불순함), 무질서 정도
    entropy = -p1 log(p1) - p2 log(p2)- .... (낮을수록 좋다(순수함))

  - 정보 증가량(Information Gain)
   - 어떤 속성에 따라 분류하면 전체 그룹의 entropy가 감소되는지 측정
   - 새로 추가된 정보에 따른 엔트로피의 변화를 추정
   - **IG(부모, 자식) = etropy(부모)- [P1(자식1) entropy(자식1) + P2(자식2) entropy(자식2)+.....]

예를들어) 한 박스에 ★이 14개 ●이 16개 총 30개가 있다 p(★)는 별이 나올 확률은 16/30=0.53 p(●)이 나올 확률은 14/30 = 0.47이다.
부모 entropy는 entropy(부모) = -[ p(●) log( p(●)) +  p(★) log( p(★))] = 1[0.53 -0.9 + 0.47 -1.1] = 0.99(매우 불순하다)
부모 박스에서 분리가 되서 p(★)는 1개 p(●)는 12개 각각 확률값은 0.08, 0.92이다 위처럼 자식entropy를 계산하면 0.79, 0.39가 나온다 이 결과로 
information Gain을 구해보자 
IG(부모) -[p(첫번째 자식) * entropy(첫번째자식) + p(두번째 자식)* entropy(두번째 자식)] = 0.99-[0.43 *0.39 + 0.57 *0.79] = 0.37 이나온다 
이런식으로 각각 속성들을 한번씩 해보는거다. 