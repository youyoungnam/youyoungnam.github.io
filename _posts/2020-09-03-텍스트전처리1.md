---
title: 자연어처리_교육_3일차(linear_regression포함)
categories: deeplearning_study
author_profile: true
---


TF/IDF 알고리즘 기반 키워드 추출
 - 특정 문서 내에서 단어 빈도가 노퓨을 수록, 그리고 전체 문서들 중 그 단어를 포함한 문서가 적을 수록 TF-IDF값이 높아진다.
 - 모든 문서에 흔하게 나타나는 단어를 걸러내는 효과를 얻을 수 있다.


TextRank 알고리즘 기반 키워드 추출
 - google PageRank 알고리즘을 텍스트 데이터에 적용
 - 중요도가 높은 웹사이트는 다른 웹사이트들로 부터 많은 인바운드 링크를 갖는다는 점에 착안
 - 그래프 모델 단어가 vertex(node), 특정 단어를 중심으로 co-occurrence 윈도우 안에 존재하는 다른 단어와의 edge생성
 - **Connection이 많은 vertex에 높은 스코어를 부여**
 - 나중에 수학적인 수식 찾아볼것
 예를들어 
  - 현대자동차는 국내 자동차 기업이다.
  - 현대자동차는 자동차 기술을 보유하고 있다.
  - 현대자동차를 중심으로 co-occurrence 윈도우 안에 존재하는 다른 단어와 edge생성


텍스트 벡터화
 - 텍스트는 자연어 토큰열로 구성된 데이터이므로 실수 형태의 자료를 입력으로 받는 수학적 모델에 사용할 수 없음.
 - 단어나 문장이 갖는 의미 정보를 실수타입 정보 배열, 즉, 벡터로 변환하는 방법이 필요

 - 종류
   - 빈도 기반 벡터화
    - 문장이나 문서에 출현한 단어의 빈도를 벡터에 반영
    - **실제로 중요성이 낮어 단어임에도 불구하고 큰 값을 부여받아 정보 왜곡의 원인이 될 수 있음**
    - **텍스트 데이터가 의미를 전달하는 객체이므로 의미적인 특징 정보가 벡터에 반영되어야 하지만 출현 빈도만으로는 의미적인 정보 반영이 어려움**

   - Bag-of-word 모델(One-hot인코딩)
    - 전체문서에 출현한 단어(토큰)의 리스트 생성.
    - 개별 문서의 특정 토큰 출연 유무에 따라 0또는1 대입하여 N차원 벡터 공간의 벡터로 변환.
    - 토큰 간의 중요도 차이를 반영하지 못함
    - 단어의 수만큼 벡터의 차원이 늘어남. Curse of Dimensionality + sparse vector
    - 예를들어
      - John likes to watch movies.
      - 1. John 2 likes 3 to 4 watch 5 movies 이런식으로 늘어남
   - TF-IDF 벡터화
    - Bagofword방식의 벡터화는 단어의 중요도에 대한 정보 손실
    - **TF-IDF 통계량은 특정 단어의 중요도 정보를 나타내므로 선형모델의 가중치 매개변수 튜닝에 적합**



    유클리드 거리 기반 유사도 계산
     - 유클리드 공간 좌표계에서 두 점(벡터) 사이의 직선 거리로 유사도 측정.
     - L2 Norm에 대응
     - 벡터 공간 상에서 근접한 거리에 위치할수록 작은 값.
     - **벡터의 magnitude가 고려되므로 길이가 다른 두 텍스트를 Bag of Words와 같은 방식으로 벡터 공간에 텍스트를 매핑할 경우 적절한 비교가 될 수 없음**


     코사인 유사도 계산
      - 내적 공간에서 두 벡터간의 코사인각 으로 유사도 산출
      - BagOfwords 모델이나 CountVectorizer에 의한 sparse vector를 사용할 경우 낮은 품질
      - GloVe와 같은 Pre-trained 워드 임베딩 모델과 함께 사용할 경우 높은 품질.

**벡터화(Vectorizing)**
  - 자연어를 기계가 이해할 수 있도록 숫자로 변환해주는 과정
  - 컴퓨터는 내부에서 모두 숫자로 이루어져있음

**원-핫 인코딩(One-Hot Encoding)**
  - 단어의 집합 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스를 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 벡터 표현 방식
  - 원-핫 과정
    - 각 단어에 고유한 인덱스 부여 
    - 표현하고 싶은 단어에 1, 아닌 단어에 0을 부여


**원핫인코딩(코드)**
```python
text = "Hello, Python you can do it coding"

# 단어 토큰화 먼저 

# 문장분리 후 단어 토큰화 
import nltk
from nltk.tokenize import sent_tokenize
from nltk import WordPunctTokenizer

sentences = sent_tokenize(text)
tokens = [WordPunctTokenizer().tokenize(word) for word in sentences]

def make_vocab(token):
    word2index = {}

    for word in token:
        if word not in word2index:
            word2index[word] = len(word2index)
    return word2index

def one_hot_encoding(wordindex, wor):
    one_hot = [0] * len(wordindex)
    index = wordindex[wor]
    one_hot[wor] = 1
    return one_hot
```

**과적합(Overfitting)**
 - 훈련 데이터를 과하게 학습한 상태
 - 훈련 데이터에서 정확도는 매우 높지만, 실제 서비스에서의 데이터는 정확도가 좋지 않는 현상

**과소적합(Underffitting)**
 - 테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한상태
 - 대표적인 예로 Word2Vec가있다.


 **단순 선형 회귀 분석(Simple Linear Regression Analysis)**
  - 데이터가 선형 경향을 뛴다고 가정
  - y = w *x + b 
  - x: 독립변수 w:가중치(weight) b: 편향(bias)
  - 가설에 대해서 적절한 w와 b를 찾아주는 형태 
 
  - **비용함수(Cost function or Loss function)**
    - H(x) = W*x +b
    - W = 2 b = 2 가정 
    - x =1 을 대입하면 y = 2
    - x = 2을 대입하면 y =3
    - **학습은 차이를 줄이는 방향으로  W와 b를 조정**

    **그렇다면 비용함수는 가설과 실제값의 차이를 줄이면서 학습을하는데 만약 가설값이 -1 이고 실제값이 1이면 0 ?? 나온다. 해결방법은?
    절대값, 제곱 (H(x) - y)^2**

    제곱을 취하면 차이가 큰경우에 대해서 가중치를 높아주는 효과를 봄 

    - Cost function = 1/n sum(H(x)- y_i)^2

    - 차이를 제곱한 것들의 평균
    - 평균 제곱 오차(Mean Squared Error)
    - 단순 선형 회귀분석에서는 MSE가 많이 사용됨
    - MAE, MSLE, Cross-entropy등 다양한 cost function이 존재 상황에 맞는 loss function을 선택이 필요 


    - 경사하강법(Gradient Descent)
     - **임의의 W값을 정한 뒤에, cost function가 최소가 되도록 W를 조금씩 수정하는 방법**
     - 미분을 통한 접선에서의 기울기를 활용
     - 학습률(learning_rate) W의 값을 변경할때, 얼마나 크게 변할지 결정 
       - 너무 크게 변하면 발산, 너무 작게 변하면 학습 속도가 느려짐
