---
title: 자연어처리_교육_3일차
categories: deeplearning_study
author_profile: true
---


TF/IDF 알고리즘 기반 키워드 추출
 - 특정 문서 내에서 단어 빈도가 노퓨을 수록, 그리고 전체 문서들 중 그 단어를 포함한 문서가 적을 수록 TF-IDF값이 높아진다.
 - 모든 문서에 흔하게 나타나는 단어를 걸러내는 효과를 얻을 수 있다.


TextRank 알고리즘 기반 키워드 추출
 - google PageRank 알고리즘을 텍스트 데이터에 적용
 - 중요도가 높은 웹사이트는 다른 웹사이트들로 부터 많은 인바운드 링크를 갖는다는 점에 착안
 - 그래프 모델 단어가 vertex(node), 특정 단어를 중심으로 co-occurrence 윈도우 안에 존재하는 다른 단어와의 edge생성
 - **Connection이 많은 vertex에 높은 스코어를 부여**
 - 나중에 수학적인 수식 찾아볼것
 예를들어 
  - 현대자동차는 국내 자동차 기업이다.
  - 현대자동차는 자동차 기술을 보유하고 있다.
  - 현대자동차를 중심으로 co-occurrence 윈도우 안에 존재하는 다른 단어와 edge생성


텍스트 벡터화
 - 텍스트는 자연어 토큰열로 구성된 데이터이므로 실수 형태의 자료를 입력으로 받는 수학적 모델에 사용할 수 없음.
 - 단어나 문장이 갖는 의미 정보를 실수타입 정보 배열, 즉, 벡터로 변환하는 방법이 필요

 - 종류
   - 빈도 기반 벡터화
    - 문장이나 문서에 출현한 단어의 빈도를 벡터에 반영
    - **실제로 중요성이 낮어 단어임에도 불구하고 큰 값을 부여받아 정보 왜곡의 원인이 될 수 있음**
    - **텍스트 데이터가 의미를 전달하는 객체이므로 의미적인 특징 정보가 벡터에 반영되어야 하지만 출현 빈도만으로는 의미적인 정보 반영이 어려움**

   - Bag-of-word 모델(One-hot인코딩)
    - 전체문서에 출현한 단어(토큰)의 리스트 생성.
    - 개별 문서의 특정 토큰 출연 유무에 따라 0또는1 대입하여 N차원 벡터 공간의 벡터로 변환.
    - 토큰 간의 중요도 차이를 반영하지 못함
    - 단어의 수만큼 벡터의 차원이 늘어남. Curse of Dimensionality + sparse vector
    - 예를들어
      - John likes to watch movies.
      - 1. John 2 likes 3 to 4 watch 5 movies 이런식으로 늘어남
   - TF-IDF 벡터화
    - Bagofword방식의 벡터화는 단어의 중요도에 대한 정보 손실
    - **TF-IDF 통계량은 특정 단어의 중요도 정보를 나타내므로 선형모델의 가중치 매개변수 튜닝에 적합**



    유클리드 거리 기반 유사도 계산
     - 유클리드 공간 좌표계에서 두 점(벡터) 사이의 직선 거리로 유사도 측정.
     - L2 Norm에 대응
     - 벡터 공간 상에서 근접한 거리에 위치할수록 작은 값.
     - **벡터의 magnitude가 고려되므로 길이가 다른 두 텍스트를 Bag of Words와 같은 방식으로 벡터 공간에 텍스트를 매핑할 경우 적절한 비교가 될 수 없음**


     코사인 유사도 계산
      - 내적 공간에서 두 벡터간의 코사인각 으로 유사도 산출
      - BagOfwords 모델이나 CountVectorizer에 의한 sparse vector를 사용할 경우 낮은 품질
      - GloVe와 같은 Pre-trained 워드 임베딩 모델과 함께 사용할 경우 높은 품질.

