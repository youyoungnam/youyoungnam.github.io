---
title: Pytorch에서 Permute을 사용하는이유
categories: Pytorch
author_profile: true
---



**Pytorch를 공부하다가 Permute 메서드가 나왔는데 이해가 잘 되지않는다 그래서 정리가 필요했다.**


**Pytorch의 randn 메서드 함수를 사용해 평균이 0 표준편차가1인 정규분포에서 샘플링한 값을 만든다**

```python
import torch


x = torch.randn(16, 32, 3)
print("x shape: ", x.shape)
print("첫 번째 데이터 형태: ", x[0].shape)
print("전체 데이터 갯수: ", len(x), "\n전체 데이터중 첫 번째 데이터의 수: ",len(x[0]))
>>
x shape:  torch.Size([16, 32, 3])
첫 번째 데이터 형태:  torch.Size([32, 3])
전체 데이터 갯수:  16 
전체 데이터중 첫 번째 데이터의 수:  32
```
**말로 해석해보자면 평균이 0이고 표준편차가 1인 정규분포에서 샘플링을 하는데 총 16개의 데이터셋을 만들었고, 각 데이터는 32개씩 존재하고 3차원인 형태이다.**

**permute 메서드로 조작을 해보자!**


```python
# 32, 16, 3
x1 = x.permute(1, 0, 2)
print("x1 shape: ", x1.shape)
print("전체 데이터 갯수: ", len(x1),"\n전체 데이터중 첫 번째 데이터의 수: ",x1[0].shape)
>>
x1 shape:  torch.Size([32, 16, 3])
전체 데이터 갯수:  32 
전체 데이터중 첫 번째 데이터의 수:  torch.Size([16, 3])
```


**내가 궁금한점은 이런식으로 학습을 하게되면 데이터 정보가 사라지는게 아닐까? 의문이든다 궁금하다 구글링을 하게되면 종종 channel을 마지막으로 보내야하는 순간이나, 차원간 순서를 바꿔줘야 할 때 사용한다고 한다.
그렇다면 바꾸고 난뒤에 학습은 안하는걸까? 단순 연산때문에 바꿔주는것인가??** 