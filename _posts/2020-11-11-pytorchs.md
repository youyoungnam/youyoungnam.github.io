---
title: Pytorch를 이용해서 BackPropagation구현해보기
categories: Pytorch
author_profile: true
---

```python
#pytorch에서 Back Propagation을 이용해 파라미터를 업데이트하는 밥언은 Autograd방식으로 쉽게 구현할 수 있도록 설정 되어 있음

import torch

if torch.cuda.is_available():
    DEVICE = torch.device('cuda')

else:
    DEVICE = torch.device('cpu')
```

```python
# batch_size는 데이터를 학습할 때 얼만큼 씩 학습을 할것인가를 의미 예를들어 
# 1000개 데이터가 있다고 가정 5epch , batch_size를 10일 때 10 * 100 = 1000 10개의 데이터를 100번 학습하면 1 epoch 전체 학습횟수  즉, input으로 이용되는 데이터가 64개라고 이해하면된다.
BATCH_SIZE = 64

# INPUT_SIZE는 딥러닝 모델에서의 input의 크기이자 입력층의 노드 수를 의미 여기서 input size가 1000이므로 입력 데이터의 크기가 1000이라는것을 의미 즉 1000크기의 벡터 값을 의미 

# batch_size랑 종합해보면, 1000개의 크기의 벡터 값을 64개를 이용한다는것 shape로 보면, (64, 1000)
INPUT_SIZE = 1000

#hidden size는 딥러닝 모델에서 input을 다수의 파라미터를 이용해 계산한 결과에 한번 더 계산되는 파라미터 수 즉, 입력측에서 은닉층으로 전달됐을 때 은닉층의 노드 수를 의미 여기서 입력층은 (64, 1000) input들이 (1000, 100)크기의 행렬과 행렬 곱으로 계산
HIDDEN_SIZE= 100


#output size는 딥러닝 모델에서 최종으로 출력되는 값의 벡터의 크기를 의미 즉, 예측해야한 label 갯수 
# 예를들어 10개를 예측해야한다면 10 5개를 예측해야한다면 5 

OUTPUT_SIZE=10

X = torch.randn(BATCH_SIZE,
                INPUT_SIZE,
                device= DEVICE,
                dtype = torch.float,
                requires_grad = False)
y = torch.randn(BATCH_SIZE,
                OUTPUT_SIZE,
                device = DEVICE,
                dtype = torch.float,
                requires_grad=False)
w1 = torch.randn(INPUT_SIZE,
                 HIDDEN_SIZE,
                 device = DEVICE,
                 dtype= torch.float,
                 requires_grad =True)
w2 = torch.randn(HIDDEN_SIZE,
                 OUTPUT_SIZE,
                 device = DEVICE,
                 dtype = torch.float,
                 requires_grad = True)
```
앞에서 batch_size, input_size, hidden_size, output_size를 정의했음
첫 번째로 임포트한 torch와 torch.randn 메서드를 이용해 데이터와 파라미터를 설정 
torch.randn은 평균이 0 표준편차가 1인 정규분포에서 샘플링한 값으로 데이터를 만든다는 것을 의미 데이터를 만들어 줄 때 
데이터의 모양을 설정할 수 있다. 즉, 크기가 1000짜리의 벡터를 64개 만들기 위해 batch_size64, input_size 1000
설정했으며, X는 (64, 1000) 모양의 데이터가 생성 된다.
이때 생성된 데이터는 device를 이용해 계산할 것이기 때문에 device확인
데이터 형태는 float형태 또한 해당 데이터는 input으로 Gradient는 업데이트 할 필요가 없기 때문 False


y output도  input을 설정하는 것과 동일 output 역시 bacth_size 수만큼 결과 값이 필요하며 output과의 오차를 계산하기 위해 output의 크기를 10으로 설정 device, dtye Gadient는 input과 동일 



이제 본격적으로 업데이트할 파라미터 값을 설정 앞에서 input과 output을 설정한 내용과 동일
여기서 w1은 input의 데이터 크기가 1000dlau 이와 행렬 곱을 하기위해 다음 행의 값이 1000이어야함 
또한 행렬 곱을 이용해 100 크기의 데이터를 생성하기 위해 (1000, 100)크기의 데이터를 생성 divice, dtype은 동일 
required_grad는 True Gradient가 업데이트 되기 위해서 



w2는 w1과 x를 행렬 곱한 결과에 계산할 수 있는 데이터여야 한다 w1과 x의 행렬 곱을 한 결과는 (1, 100)이며 (100, 10)
행렬을 통해 output을 계산할 수 있도록 w2의 모양을 설정하자 hidden_size는 100 output_size는 10 
최종은 (1,10) 
w2 역시 Back Propagation을 통해 업데이트해야 하는 대상이므로 requires_gard는 True로 설정 




```python
learning_rate =1e-6
print("X.shape", X.shape)
for t in range(1, 501):
    y_pred = X.mm(w1).clamp(min= 0).mm(w2)

    loss = (y_pred - y).pow(2).sum()

    if t % 100 ==0:
        print("Iteration: ", t, "Loss: ", loss.item())
    loss.backward()


    with torch.no_grad():
        w1 -= learning_rate * w1.grad
        w2 -= learning_rate * w2.grad


        w1.grad.zero_()
        w2.grad.zero_()


```

파라미터를 업데이트할 때, Gradient를 계산한 결괏값에 learning_Rate만큼 곱한 값을 이용해 업데이트된다.
이를 Learning_rate라고 한다. learning_rate를 어떻게 설정하느냐에 따라 Gradient 값에 따른 학습 정도가 결정
딥러닝 모델에서 파라미터 값을 업데이트할 때 가장 중요한 하이퍼파라미터(hyper_parameter)이기도 함


500번 반복해 파라미터 값을 업데이트하기 위해 반복문을 설정 t값이 1부터 500번 반복 되면서 아래에 작성된 코드가 실행
딥러닝 모델의 결갓값을 보통 예측값이라고 표현 딥러닝 모델의 Input인 x와 parameter w1간의 행렬 곱을 이용해 나온 결괎을 
계산 그 이후 torch모듈 내 clamp라는 method를 이용해 비선형 함수를 적용 딥러닝 모델에서는 층과 층 사이에 비선형 함수를 
이용해 높은 표현력을 지니는 방정식을 얻게 됨 여기서 clamp는 비선형 함수 ReLU와 같은 역할을 함
최솟값이 0이며 0보다 큰 값은 자기 자신을 갖게되는 메서드 이기 때문 clamp를 이용해 계산된 결과와 
w2를 이용해 행렬 곱을 한 번 더 계싼 
행렬 곱을 한 결과는 딥러닝 모델에서의 output을 의미하며 이는 예측값이라고 표현되기 때문에 y_pred라고 설정
clamp에 대한 식 


예측값과 실제 레이블 값을 비교해 오차를 계산한 값을 Loss라고 함 
예측값을 의미하는 y_rped와 실제 레이블을 의미하는 y간의 차잇값을 계산한 후 Torch Module내 pow함수를 이용해 제곱을 취함
pow() method는 지수를 취하는 기본 메서드 즉, y_pred y.pow(2) 제곱차를 의미하며 제곱차의 합을 sum()을 이용해 계산


반복 횟수를 의미하는 t가 100으로 나누어 떨어질 때 현재 진행 중인 반복문 횟수와 loss값을 출력과정

계산된 Loss 값에 대해 bachword 메서드를 이용하면 각 파라미터 값에 대해 Gradient를 계산하고 이를 통해 Back Propagation
을 진행한다는 것을 의미한다 파이토치 내에서 Back Propagation을 쉽게 진행 할 수 있도록 해줌


각 파라미터 값에 대해 gradient를 계산한 결과를 이용해 파라미터 값을 업데이트할 때는 해당 시점의 gradient값을 고정한 후
업데이트를 진행 코드가 실행되는 시점에서 Gradient값을 고정한다는 의미

Gradient값을 고정한 상태에서 w1의 Gradient값을 의미하는 w1.grad 설정한 learning_rate갑승ㄹ 곱한 결과값을 기존 w1에서 빼줌
음수를 해주는 이유는 Loss값이 최소로 계산될 수 있는 파라미터 값을 찾기 위해 Gradient값에 대한 반대 방향으로 계산

w2에도 마찬가지로 w2.grad위와 같은방법으로 진행


각 파라미터 값을 업데이트 했다면 각 파라미터 값의 gradient를 초기화해 다음 반복문을 진행할 수 있도록 Gradient값을 0으로
설정 w1, w2 각각에 대해 동일하게 grad.zero() 메서드를 적용해 Gradient값을 0으로 설정 
왜냐하면 BackProgation을 진행할 때 gradient값을 loss.backward()을 통해 새로 계산되기 때문이다 