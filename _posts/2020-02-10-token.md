---
title: 토큰화(Tokenization)
categories: NLP
author_profile: true
---
## 시작하기전에
---

*토큰화를 하기전에 토큰화가 무엇인지 알아보자. 토큰화가 무엇일까? 코퍼스(corpus)에서 토큰이라 불리는 단위로 나누는 작업을 토큰화 라고한다.*

*쉽게 말하자면 의미있는 단위로 나누는 작업이라고 생각하면 된다.*

- **토큰화(Tokenization):** 의미있는 단위로 나누는 작업
- **코퍼스(corpus):** 글 또는 말 텍스트를 모아 놓은것

*토큰화 중에서는 단어 토큰화(Word Tokenization), 문장 토큰화(Sentence Tokenization)이 있다 

---

### 단어 토큰화(Word Tokenization)
토큰의 기준을 단어로 하는것이다. 위에 보면 토큰화는 의미있는 단위로 나누는 작업이라고 했다. 그단위가 단어 기준이다. 

**사용법**

```python

from nltk.tokenize import word_tokenize
text ="Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."

print(word_tokenize(text))
```
>['Do', "n't", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', "'s", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']

위에 작업을 보면 토큰화가 이루어진걸 볼 수 있다. 하지만 제대로 처리되지 못한게 몇개 보인다. 

Dot't를 Do, n't 따로 구분했고 
Mr.Jone's를 Mr, Jone, 's 따로따로 구분해버린다 

그렇다면 같은문장을 다른 도구로 사용해보자.

```python
from nltk.tokenize import WordPunctTokenizer
text ="Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop." 
print(WordPunctTokenizer(text))
```
>['Don', "'", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', "'", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']

WordPunctTokenizer도구는 
- Don't -> Don, ', t
- Mr.Jone's -> Mr, ., Jone, ', s 
이렇게 구분이 된다.

또 다른 도구는 어떤식으로 구분이 될까? 이번에는 케라스로 토큰화를 해볼것이다.

```python
from tensorflow.keras.preprocessing.text import text_to_word_sequence
text = "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."
print(text_to_word_sequence(text))
```
>["don't", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', "jone's", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']

이번에는 Don't도 Mr.Jone's도 제대로 구분된걸 볼 수 있다.

---

### 문장 토큰화(Sentence Tokenization)
이번에 토큰화 단위는 문자으로 나누는것이다. 다시 위에처럼 어떻게 사용하는지 알아볼것이다.
**사용법**

```python
from nltk.tokenize import sent_tokenize
text = "His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near."

print(sent_tokenize(text))
```
>['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to mae sure no one was near.']

토큰화 결과를 보면 온점(.)을 기준으로 토큰화가 된걸 볼 수있습니다. 그렇다면 온점(.)이 여러번 등장하면 어떻게 구분할까? 

```python
text = "I am actively looking for Ph.D. students. and you are a Ph.D student."
print(sent_tokenize(text))
```

>['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']

이 NLTK는 결과를 보면 단순히 온점(.)으로만 문장을 구분하지 않은게 보인다.


위에서는 영어로 토큰화를 했다. 그러면 한국어는 토큰화를 못하는건가? 못하는건 아니고 영어 보다는 어렵다 



### 한국어 토큰화 어려움
---

*왜 한국어는 토큰화가 어려울까? 영어는 New york이나 he's와 같이 줄임말에 대한 예외처리를 해준다면 띄어쓰기 기준(whitespace)를 기준으로 하는 띄어쓰기 토큰화를 수행해도 단어 토큰화가 잘 작동한다.

**왜?** 거의 대부분 경우 단어 띄어쓰기가 이루어지기 때문에 띄어쓰기 토큰화가 거의 같기때문이다.

한국어는 단순히 띄어쓰기로만 토큰화를 하기에 부족하다.
왜냐하면 한국어 NLP에서는 어절 토큰화를 지양하고 있기때문이다.

> **어절:** 띄어쓰기 단위가 되는 단위를 어절이라고한다


**그러면 한국어는 왜 토큰화가 어려울까?**

**1. 한국어는 교착어다**
한국어는 영어와는 달리 조사라는게 존재한다. 예를들어 그(he/him)이라는 주어 목적어가 들어가는 문장이 있다고 생각해보자.  그라는 단어 하나에도 그가, 그에게, 그를, 그와, 다양한 조사가 그 라는 글자뒤에 띄어쓰기 없이 바로 붙기때문이다.

즉, 띄어쓰기가 단위인 영어처럼 독립적인 단어라면 띄어쓰기 단위로 토큰화를 하면 되지만, 한국어는 어절이라는 독립적인 단어로 구성되는 것이 아니라 조사 등의 무언가가 붙어있는 경우가 많아서 이를 전부 처리 해줘야 한다는 의미이다.

한국어 토큰화에서는 형태소(morpheme)라는 개념을 이해 하고 있어야한다. 
> **형태소:** 의미를 가지는 가장 최소단위를 말한다

형태소에는 자립형태소, 의존형태소가 존재한다

- **자립형태소:** 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소
- **의존형태소:** 다른 형태소와 같이 결합하여 사용되는 형태소 접사, 어미, 조사, 어간

예를들어 
- 문장: 에디가 딥러닝책을 읽었다.

- 자립형태소: 에디, 딥러닝책
- 의존형태소: -가, -을, -읽, -었, -다

한국어 NLP에서 형태소 분석기를 사용한다는 것은 단어 토큰화가 아니라 정확히 형태소 단위로 형태소 토큰화를 수행 해야한다. 

```python
from konlpy.tag import Okt
okt =Okt()
print(okt.morphs("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))

print(okt.pos("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))

print(okt.nouns("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
```

>['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요'] 

>[('열심히','Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')] 

>['코딩', '당신', '연휴', '여행'] 

위에는 Okt 형태소 분석기로 토큰화를 해본 결과이다.

1. morphs :형태소 추출
2. pos: 품사 태깅
3. nouns: 명사 추출


위에 konlpy을 사용하려면 konlpy를 다운로드를 해야한다. 

**위에 내용은 [딥러닝을 이용한 자연어처리 입문](https://wikidocs.net/book/2155) 여기서 공부한 내용을 쓴것이다.**

